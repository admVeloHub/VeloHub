// AI Service - Integra√ß√£o h√≠brida com IA para respostas inteligentes
// VERSION: v2.1.0 | DATE: 2025-01-27 | AUTHOR: Lucas Gravina - VeloHub Development Team
const { OpenAI } = require('openai');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const config = require('../../config');

class AIService {
  constructor() {
    this.openai = null;
    this.gemini = null;
    this.openaiModel = "gpt-4o-mini"; // Modelo otimizado para custo (fallback)
    this.geminiModel = "gemini-2.5-pro"; // Modelo Gemini 2.5 Pro (prim√°rio)
  }

  /**
   * Inicializa o cliente OpenAI apenas quando necess√°rio
   */
  _initializeOpenAI() {
    if (!this.openai && this.isOpenAIConfigured()) {
      this.openai = new OpenAI({
        apiKey: config.OPENAI_API_KEY,
      });
    }
    return this.openai;
  }

  /**
   * Inicializa o cliente Gemini apenas quando necess√°rio
   */
  _initializeGemini() {
    if (!this.gemini && this.isGeminiConfigured()) {
      this.gemini = new GoogleGenerativeAI(config.GEMINI_API_KEY);
    }
    return this.gemini;
  }

  /**
   * Gera resposta inteligente baseada na pergunta e contexto
   * FLUXO: Gemini (prim√°rio) ‚Üí OpenAI (fallback) ‚Üí Resposta padr√£o
   * @param {string} question - Pergunta do usu√°rio
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @param {string} userId - ID do usu√°rio
   * @param {string} email - Email do usu√°rio
   * @returns {Promise<Object>} Resposta com provider usado
   */
  async generateResponse(question, context = "", sessionHistory = [], userId = null, email = null) {
    try {
      // 1. TENTAR GEMINI PRIMEIRO (IA PRIM√ÅRIA)
      if (this.isGeminiConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Tentando Gemini (prim√°rio) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithGemini(question, context, sessionHistory, userId, email);
          return {
            response: response,
            provider: 'Gemini',
            model: this.geminiModel,
            success: true
          };
        } catch (geminiError) {
          console.warn('‚ö†Ô∏è AI Service: Gemini falhou, tentando OpenAI fallback:', geminiError.message);
        }
      }

      // 2. FALLBACK PARA OPENAI (IA SECUND√ÅRIA)
      if (this.isOpenAIConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Usando OpenAI (fallback) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithOpenAI(question, context, sessionHistory, userId, email);
          return {
            response: response,
            provider: 'OpenAI',
            model: this.openaiModel,
            success: true
          };
        } catch (openaiError) {
          console.error('‚ùå AI Service: OpenAI tamb√©m falhou:', openaiError.message);
        }
      }

      // 3. SE AMBOS FALHARAM
      throw new Error('Nenhuma API de IA dispon√≠vel');
      
    } catch (error) {
      console.error('‚ùå AI Service Error:', error.message);
      
      // 4. FALLBACK PARA RESPOSTA PADR√ÉO
      return {
        response: `Desculpe, n√£o consegui processar sua pergunta no momento. 
        Por favor, tente novamente ou entre em contato com nosso suporte.`,
        provider: 'Fallback',
        model: 'none',
        success: false,
        error: error.message
      };
    }
  }

  /**
   * Gera resposta usando Gemini (IA PRIM√ÅRIA)
   */
  async _generateWithGemini(question, context, sessionHistory, userId, email) {
    const gemini = this._initializeGemini();
    if (!gemini) {
      throw new Error('Falha ao inicializar cliente Gemini');
    }

    const model = gemini.getGenerativeModel({ model: this.geminiModel });
    
    // Construir prompt completo (system + user) otimizado para Gemini
    const systemPrompt = `### PERSONA
Voc√™ √© o VeloBot, assistente oficial da Velotax. Responda com base no hist√≥rico de conversa e no contexto da base de conhecimento fornecidos. Sua fun√ß√£o √© formatar a resposta de forma que ela fique apropriada e profissional para o uso no atendimento a clientes. Sua resposta dever√° ser diretamente a resposta esperada, sem "aberturas", concorrdancias com a solicita√ß√£o, informa√ß√£o de que compreendeu nem nada do g√™nero. 

### REGRAS
- Se a nova pergunta for amb√≠gua, use o hist√≥rico para entender o que o usu√°rio quis dizer.
- Seja direto e claro, mas natural e prestativo.
- Se o usu√°rio disser "n√£o entendi", reformule sua √∫ltima resposta de forma mais simples.
- Se n√£o encontrar no contexto, diga: "N√£o encontrei essa informa√ß√£o na base de conhecimento dispon√≠vel."
- Sempre responda em portugu√™s do Brasil.
- Use o contexto fornecido para dar respostas precisas e relevantes.`;

    const userPrompt = this.buildOptimizedPrompt(question, context, sessionHistory);
    
    // Combinar system + user prompt para Gemini
    const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;
    
    console.log(`ü§ñ Gemini: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const result = await model.generateContent(fullPrompt);
    const response = result.response.text();
    
    console.log(`‚úÖ Gemini: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Gera resposta usando OpenAI (IA FALLBACK)
   */
  async _generateWithOpenAI(question, context, sessionHistory, userId, email) {
    const openai = this._initializeOpenAI();
    if (!openai) {
      throw new Error('Falha ao inicializar cliente OpenAI');
    }

    // Construir prompt otimizado (baseado no chatbot Vercel)
    const prompt = this.buildOptimizedPrompt(question, context, sessionHistory);
    
    console.log(`ü§ñ OpenAI: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const completion = await openai.chat.completions.create({
      model: this.openaiModel,
      messages: [
        {
          role: "system",
          content: `### PERSONA
Voc√™ √© o VeloBot, assistente oficial da Velotax. Responda com base no hist√≥rico de conversa e no contexto da base de conhecimento.

### REGRAS
- Se a nova pergunta for amb√≠gua, use o hist√≥rico para entender o que o usu√°rio quis dizer.
- Seja direto e claro, mas natural e prestativo.
- Se o usu√°rio disser "n√£o entendi", reformule sua √∫ltima resposta de forma mais simples.
- Se n√£o encontrar no contexto, diga: "N√£o encontrei essa informa√ß√£o na base de conhecimento dispon√≠vel."
- Sempre responda em portugu√™s do Brasil.
- Use o contexto fornecido para dar respostas precisas e relevantes.`
        },
        {
          role: "user",
          content: prompt
        }
      ],
      temperature: 0.2,
      max_tokens: 1024,
    });

    const response = completion.choices[0].message.content;
    
    console.log(`‚úÖ OpenAI: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Constr√≥i o prompt otimizado com contexto e hist√≥rico
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Prompt formatado
   */
  buildOptimizedPrompt(question, context, sessionHistory) {
    let prompt = `
### HIST√ìRICO DE CONVERSA
${sessionHistory.length > 0 ? 
  sessionHistory.map(h => `${h.role}: ${h.content}`).join("\n") : 
  'Primeira pergunta da sess√£o.'}

### CONTEXTO DA BASE DE CONHECIMENTO
${context || "Nenhum contexto espec√≠fico encontrado."}

### PERGUNTA ATUAL
"${question}"
`;

    return prompt;
  }

  /**
   * Constr√≥i o prompt com contexto e hist√≥rico (m√©todo legado)
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Prompt formatado
   */
  buildPrompt(question, context, sessionHistory) {
    return this.buildOptimizedPrompt(question, context, sessionHistory);
  }

  /**
   * Verifica se a API OpenAI est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isOpenAIConfigured() {
    return !!config.OPENAI_API_KEY && config.OPENAI_API_KEY !== 'your_openai_api_key_here';
  }

  /**
   * Verifica se a API Gemini est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isGeminiConfigured() {
    return !!config.GEMINI_API_KEY && config.GEMINI_API_KEY !== 'your_gemini_api_key_here';
  }

  /**
   * Verifica se alguma API est√° configurada
   * @returns {boolean} Status da configura√ß√£o
   */
  isConfigured() {
    return this.isOpenAIConfigured() || this.isGeminiConfigured();
  }

  /**
   * Obt√©m status das configura√ß√µes de IA
   * @returns {Object} Status das configura√ß√µes
   */
  getConfigurationStatus() {
    return {
      gemini: {
        configured: this.isGeminiConfigured(),
        model: this.geminiModel,
        priority: 'primary'
      },
      openai: {
        configured: this.isOpenAIConfigured(),
        model: this.openaiModel,
        priority: 'fallback'
      },
      anyAvailable: this.isConfigured()
    };
  }

  /**
   * Testa a conex√£o com as APIs de IA
   * @returns {Promise<Object>} Status das conex√µes
   */
  async testConnection() {
    const results = {
      gemini: { available: false, model: this.geminiModel, priority: 'primary' },
      openai: { available: false, model: this.openaiModel, priority: 'fallback' },
      anyAvailable: false
    };

    // Teste Gemini (prim√°rio)
    if (this.isGeminiConfigured()) {
      try {
        const gemini = this._initializeGemini();
        if (gemini) {
          const model = gemini.getGenerativeModel({ model: this.geminiModel });
          const result = await model.generateContent("Teste de conex√£o");
          results.gemini.available = true;
          console.log('‚úÖ Gemini: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå Gemini: Erro no teste de conex√£o:', error.message);
      }
    }

    // Teste OpenAI (fallback)
    if (this.isOpenAIConfigured()) {
      try {
        const openai = this._initializeOpenAI();
        if (openai) {
          const completion = await openai.chat.completions.create({
            model: this.openaiModel,
            messages: [{ role: "user", content: "Teste de conex√£o" }],
            max_tokens: 10,
          });
          results.openai.available = true;
          console.log('‚úÖ OpenAI: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå OpenAI: Erro no teste de conex√£o:', error.message);
      }
    }

    results.anyAvailable = results.gemini.available || results.openai.available;
    
    if (!results.anyAvailable) {
      console.warn('‚ö†Ô∏è Nenhuma API de IA dispon√≠vel');
    }

    return results;
  }
}

module.exports = new AIService();
