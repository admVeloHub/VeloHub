// AI Service - Integra√ß√£o h√≠brida com IA para respostas inteligentes
// VERSION: v2.5.0 | DATE: 2024-12-19 | AUTHOR: VeloHub Development Team
// VERSION: v2.6.3 | DATE: 2025-01-10 | AUTHOR: Lucas Gravina - VeloHub Development Team
// VERSION: v2.7.0 | DATE: 2025-01-30 | AUTHOR: Lucas Gravina - VeloHub Development Team
// VERSION: v2.7.1 | DATE: 2025-01-30 | AUTHOR: Lucas Gravina - VeloHub Development Team
// OTIMIZA√á√ÉO: Handshake inteligente com ping HTTP + TTL 3min + testes paralelos
const { OpenAI } = require('openai');
const { GoogleGenerativeAI } = require('@google/generative-ai');
const config = require('../../config');

class AIService {
  constructor() {
    this.openai = null;
    this.gemini = null;
    this.openaiModel = "gpt-4o-mini"; // Modelo OpenAI (prim√°rio)
    this.geminiModel = "gemini-2.5-pro"; // Modelo Gemini (fallback)
    
    // Cache de status das IAs (TTL 3min - OTIMIZADO)
    this.statusCache = {
      data: null,
      timestamp: null,
      ttl: 3 * 60 * 1000 // 3 minutos em ms (otimizado)
    };
  }

  /**
   * Inicializa o cliente OpenAI apenas quando necess√°rio
   */
  _initializeOpenAI() {
    if (!this.openai && this.isOpenAIConfigured()) {
      this.openai = new OpenAI({
        apiKey: config.OPENAI_API_KEY,
      });
    }
    return this.openai;
  }

  /**
   * Inicializa o cliente Gemini apenas quando necess√°rio
   */
  _initializeGemini() {
    if (!this.gemini && this.isGeminiConfigured()) {
      this.gemini = new GoogleGenerativeAI(config.GEMINI_API_KEY);
    }
    return this.gemini;
  }

  /**
   * Gera resposta inteligente baseada na pergunta e contexto
   * FLUXO: Prim√°ria (handshake) ‚Üí Secund√°ria (fallback) ‚Üí Resposta padr√£o
   * @param {string} question - Pergunta do usu√°rio
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @param {string} userId - ID do usu√°rio
   * @param {string} email - Email do usu√°rio
   * @param {Object} searchResults - Resultados da busca h√≠brida (opcional)
   * @param {string} formatType - Tipo de formata√ß√£o (conversational, whatsapp, email)
   * @param {string} primaryAI - IA prim√°ria definida pelo handshake ('OpenAI' ou 'Gemini')
   * @returns {Promise<Object>} Resposta com provider usado
   */
  async generateResponse(question, context = "", sessionHistory = [], userId = null, email = null, searchResults = null, formatType = 'conversational', primaryAI = 'OpenAI') {
    try {
      console.log(`ü§ñ AI Service: Gerando resposta para usu√°rio ${userId || 'an√¥nimo'} - Prim√°ria: ${primaryAI}`);
      
      // 1. TENTAR IA PRIM√ÅRIA (definida pelo handshake)
      if (primaryAI === 'OpenAI' && this.isOpenAIConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Tentando OpenAI (prim√°ria) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithOpenAI(question, context, sessionHistory, userId, email, searchResults, formatType);
          return {
            response: response,
            provider: 'OpenAI',
            model: this.openaiModel,
            success: true
          };
        } catch (openaiError) {
          console.warn('‚ö†Ô∏è AI Service: OpenAI falhou, tentando Gemini fallback:', openaiError.message);
        }
      } else if (primaryAI === 'Gemini' && this.isGeminiConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Tentando Gemini (prim√°ria) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithGemini(question, context, sessionHistory, userId, email, searchResults, formatType);
          return {
            response: response,
            provider: 'Gemini',
            model: this.geminiModel,
            success: true
          };
        } catch (geminiError) {
          console.warn('‚ö†Ô∏è AI Service: Gemini falhou, tentando OpenAI fallback:', geminiError.message);
        }
      }

      // 2. FALLBACK PARA IA SECUND√ÅRIA
      if (primaryAI === 'OpenAI' && this.isGeminiConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Usando Gemini (fallback) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithGemini(question, context, sessionHistory, userId, email, searchResults, formatType);
          return {
            response: response,
            provider: 'Gemini',
            model: this.geminiModel,
            success: true
          };
        } catch (geminiError) {
          console.error('‚ùå AI Service: Gemini tamb√©m falhou:', geminiError.message);
        }
      } else if (primaryAI === 'Gemini' && this.isOpenAIConfigured()) {
        try {
          console.log(`ü§ñ AI Service: Usando OpenAI (fallback) para usu√°rio ${userId || 'an√¥nimo'}`);
          const response = await this._generateWithOpenAI(question, context, sessionHistory, userId, email, searchResults, formatType);
          return {
            response: response,
            provider: 'OpenAI',
            model: this.openaiModel,
            success: true
          };
        } catch (openaiError) {
          console.error('‚ùå AI Service: OpenAI tamb√©m falhou:', openaiError.message);
        }
      }

      // 3. SE AMBOS FALHARAM
      throw new Error('Nenhuma API de IA dispon√≠vel');
      
    } catch (error) {
      console.error('‚ùå AI Service Error:', error.message);
      
      // 4. FALLBACK PARA RESPOSTA PADR√ÉO
      return {
        response: `N√£o consegui processar sua pergunta no momento. Pode reformular sua pergunta ou fornecer mais detalhes para que eu possa ajud√°-lo melhor?`,
        provider: 'Fallback',
        model: 'none',
        success: false,
        error: error.message
      };
    }
  }

  /**
   * Obt√©m a persona padr√£o do VeloBot
   * @returns {string} Persona formatada
   */
  getPersona() {
    return `# VELOBOT - ASSISTENTE OFICIAL VELOTAX

## IDENTIDADE
- Nome: VeloBot
- Empresa: Velotax
- Fun√ß√£o: Assistente de atendimento ao cliente
- Tom: Profissional, direto, prestativo, conversacional, solid√°rio.

## COMPORTAMENTO
- Responda APENAS com a informa√ß√£o solicitada
- Seja direto, sem pre√¢mbulos ou confirma√ß√µes
- Use portugu√™s brasileiro claro e objetivo
- As intera√ß√µes esperadas s√£o de chunho textual, sem adicionar informa√ß√µes gen√©ricas, criadas, ou realizar pesquisas externas.
- Apenas os conhecimentos fornecidos s√£o v√°lidos. N√£o invente informa√ß√µes.
- N√ÉO use conhecimento externo ou associa√ß√µes que n√£o estejam nos dados fornecidos.
- Se a resposta contiver muitos termos t√©cnicos, simplifique para um n√≠vel de f√°cil compreens√£o.
- Se n√£o souber, diga: "N√£o encontrei essa informa√ß√£o na base de conhecimento dispon√≠vel"

## FONTES DE INFORMA√á√ÉO
- Base de dados: Bot_perguntas (MongoDB)
- Artigos: Documenta√ß√£o interna
- Prioridade: Informa√ß√£o s√≥lida > IA generativa

## FORMATO DE RESPOSTA
- Direto ao ponto
- Sem "Entendi", "Compreendo", etc.
- M√°ximo 200 palavras
- Foco na solu√ß√£o pr√°tica`;
  }

  /**
   * Obt√©m a persona para formata√ß√£o WhatsApp
   * @returns {string} Persona formatada para WhatsApp
   */
  getWhatsAppPersona() {
    return `# VELOBOT - REFORMULADOR WHATSAPP

## IDENTIDADE
- Nome: VeloBot
- Empresa: Velotax
- Fun√ß√£o: Reformulador de respostas para WhatsApp
- Tom: Informal, amig√°vel, direto, com emojis

## COMPORTAMENTO
- Reformule a resposta para o formato WhatsApp
- Use linguagem informal e amig√°vel
- Seja conciso e direto
- Use quebras de linha para facilitar leitura
- Evite jarg√µes t√©cnicos complexos
- Use abrevia√ß√µes comuns do WhatsApp quando apropriado

## FORMATO WHATSAPP
- M√°ximo 150 palavras
- Quebras de linha frequentes
- Emojis estrat√©gicos
- Linguagem coloquial
- Foco na praticidade

## ESTRUTURA
- Sauda√ß√£o informal (se apropriado)
- Informa√ß√£o principal
- Detalhes importantes
- Encerramento amig√°vel`;
  }

  /**
   * Obt√©m a persona para formata√ß√£o E-mail formal
   * @returns {string} Persona formatada para E-mail
   */
  getEmailPersona() {
    return `# VELOBOT - REFORMULADOR E-MAIL FORMAL

## IDENTIDADE
- Nome: VeloBot
- Empresa: Velotax
- Fun√ß√£o: Reformulador de respostas para E-mail formal
- Tom: Profissional, formal, estruturado, cort√™s

## COMPORTAMENTO
- Reformule a resposta para o formato de e-mail formal
- Use linguagem profissional e cort√™s
- Estruture a informa√ß√£o de forma clara e organizada
- Use t√≠tulos e subt√≠tulos quando apropriado
- Seja detalhado mas objetivo
- Mantenha tom respeitoso e profissional

## FORMATO E-MAIL FORMAL
- M√°ximo 300 palavras
- Estrutura clara com t√≠tulos
- Linguagem formal e cort√™s
- Detalhamento apropriado
- Foco na completude da informa√ß√£o

## ESTRUTURA
- Sauda√ß√£o formal
- Assunto/contexto
- Informa√ß√£o principal estruturada
- Detalhes relevantes
- Encerramento cort√™s
- Assinatura da empresa`;
  }

  /**
   * Obt√©m a persona baseada no tipo de formata√ß√£o
   * @param {string} formatType - Tipo de formata√ß√£o (conversational, whatsapp, email)
   * @returns {string} Persona apropriada
   */
  _getPersonaByFormat(formatType) {
    console.log(`üîç Persona Debug: formatType recebido: "${formatType}"`);
    
    switch (formatType) {
      case 'whatsapp':
        console.log('üîç Persona Debug: Selecionando persona WhatsApp');
        return this.getWhatsAppPersona();
      case 'email':
        console.log('üîç Persona Debug: Selecionando persona E-mail');
        return this.getEmailPersona();
      case 'conversational':
      default:
        console.log('üîç Persona Debug: Selecionando persona conversacional (padr√£o)');
        return this.getPersona();
    }
  }

  /**
   * Cria prompt otimizado para an√°lise eficiente
   * @param {string} question - Pergunta do usu√°rio
   * @param {Array} filteredData - Dados j√° filtrados por keywords
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Prompt otimizado
   */
  createOptimizedPrompt(question, filteredData, sessionHistory = []) {
    // 1. PERSONA E REGRAS (fixo)
    const persona = this.getPersona();
    
    // 2. PERGUNTA DO USU√ÅRIO
    const userQuestion = `Pergunta: "${question}"`;
    
    // 3. PALAVRAS-CHAVE E SIN√îNIMOS RELEVANTES (apenas os relevantes)
    const relevantKeywords = filteredData.map((item, index) => {
      return `${index + 1}. ${item.pergunta}
   Palavras-chave: ${item.palavrasChave}
   Sin√¥nimos: ${item.sinonimos}`;
    }).join('\n\n');
    
    // 4. CONTEXTO DA SESS√ÉO (se houver)
    const context = sessionHistory.length > 0 
      ? `\n\nContexto da conversa:\n${sessionHistory.slice(-3).map(msg => `- ${msg.role}: ${msg.content}`).join('\n')}`
      : '';
    
    return `${persona}

${userQuestion}

Dados relevantes:
${relevantKeywords}${context}

## TAREFA
Analise a pergunta do usu√°rio e identifique qual(is) op√ß√£o(√µes) se aplica(m):

**CRIT√âRIOS:**
- Se houver APENAS 1 op√ß√£o que responde EXATAMENTE a pergunta: retorne apenas esse n√∫mero
- Se houver M√öLTIPLAS op√ß√µes que podem responder a pergunta: retorne todos os n√∫meros separados por v√≠rgula
- Se NENHUMA op√ß√£o se aplica claramente: responda NENHUM

**IMPORTANTE:** Seja rigoroso. S√≥ retorne m√∫ltiplas op√ß√µes se realmente houver ambiguidade na pergunta.

## RESPOSTA:`;
  }

  /**
   * Analisa pergunta do usu√°rio contra base de dados usando IA (OTIMIZADO)
   * @param {string} question - Pergunta do usu√°rio
   * @param {Array} filteredData - Dados j√° filtrados por keywords
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {Promise<Object>} An√°lise da IA com op√ß√µes relevantes
   */
  async analyzeQuestionWithAI(question, filteredData, sessionHistory = []) {
    try {
      console.log(`ü§ñ AI Analyzer: Analisando pergunta: "${question}"`);
      console.log(`üîç AI Analyzer: ${filteredData.length} perguntas relevantes para an√°lise`);
      
      if (!this.isGeminiConfigured()) {
        throw new Error('IA n√£o configurada para an√°lise');
      }

      // Criar prompt otimizado
      const analysisPrompt = this.createOptimizedPrompt(question, filteredData, sessionHistory);
      
      console.log(`üìù AI Analyzer: Tamanho do prompt: ${analysisPrompt.length} caracteres`);

      const gemini = this._initializeGemini();
      const model = gemini.getGenerativeModel({ model: this.geminiModel });
      
      const result = await model.generateContent(analysisPrompt);
      const response = result.response.text().trim();
      
      console.log(`ü§ñ AI Analyzer: Resposta da IA: "${response}"`);
      console.log(`üîç AI Analyzer: Tamanho da resposta: ${response.length} caracteres`);
      
      // Verificar se a IA retornou "NENHUM" (sem matches)
      if (response.toUpperCase().includes('NENHUM') || response.trim() === '') {
        console.log('‚ùå AI Analyzer: IA retornou NENHUM - nenhuma op√ß√£o relevante identificada');
        return { relevantOptions: [], needsClarification: false, hasData: true };
      }

      // Extrair n√∫meros da resposta
      const relevantIndices = response.match(/\d+/g);
      if (!relevantIndices || relevantIndices.length === 0) {
        console.log('‚ùå AI Analyzer: Nenhuma op√ß√£o relevante identificada');
        return { relevantOptions: [], needsClarification: false, hasData: true };
      }

      // Converter para √≠ndices reais (subtrair 1)
      const indices = relevantIndices.map(num => parseInt(num) - 1).filter(idx => idx >= 0 && idx < filteredData.length);
      
      console.log(`‚úÖ AI Analyzer: ${indices.length} op√ß√µes relevantes identificadas: ${indices.join(', ')}`);
      
      // Se apenas 1 op√ß√£o relevante, n√£o precisa de esclarecimento
      if (indices.length === 1) {
        return {
          relevantOptions: [filteredData[indices[0]]],
          needsClarification: false,
          bestMatch: filteredData[indices[0]],
          hasData: true
        };
      }
      
      // M√∫ltiplas op√ß√µes = precisa de esclarecimento
      const relevantOptions = indices.map(idx => filteredData[idx]);
      
      return {
        relevantOptions: relevantOptions,
        needsClarification: true,
        bestMatch: null,
        hasData: true
      };

    } catch (error) {
      console.error('‚ùå AI Analyzer Error:', error.message);
      return { relevantOptions: [], needsClarification: false, error: error.message, hasData: true };
    }
  }

  /**
   * Gera resposta usando Gemini (IA PRIM√ÅRIA)
   */
  async _generateWithGemini(question, context, sessionHistory, userId, email, searchResults = null, formatType = 'conversational') {
    console.log(`üîç Gemini Debug: formatType recebido: "${formatType}"`);
    
    const gemini = this._initializeGemini();
    if (!gemini) {
      throw new Error('Falha ao inicializar cliente Gemini');
    }

    const model = gemini.getGenerativeModel({ model: this.geminiModel });
    
    // Construir prompt completo (system + user) otimizado para Gemini
    const systemPrompt = this._getPersonaByFormat(formatType);
    console.log(`üîç Gemini Debug: Persona selecionada para formatType "${formatType}"`);

    const userPrompt = this.buildOptimizedPrompt(question, context, sessionHistory, searchResults);
    
    // Combinar system + user prompt para Gemini
    const fullPrompt = `${systemPrompt}\n\n${userPrompt}`;
    
    console.log(`ü§ñ Gemini: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const result = await model.generateContent(fullPrompt);
    const response = result.response.text();
    
    console.log(`‚úÖ Gemini: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Gera resposta usando OpenAI (IA FALLBACK)
   */
  async _generateWithOpenAI(question, context, sessionHistory, userId, email, searchResults = null, formatType = 'conversational') {
    const openai = this._initializeOpenAI();
    if (!openai) {
      throw new Error('Falha ao inicializar cliente OpenAI');
    }

    // Construir prompt otimizado (baseado no chatbot Vercel)
    const prompt = this.buildOptimizedPrompt(question, context, sessionHistory, searchResults);
    
    console.log(`ü§ñ OpenAI: Processando pergunta para usu√°rio ${userId || 'an√¥nimo'}`);
    
    const completion = await openai.chat.completions.create({
      model: this.openaiModel,
      messages: [
        {
          role: "system",
          content: this._getPersonaByFormat(formatType)
        },
        {
          role: "user",
          content: prompt
        }
      ],
      temperature: 0.1, // Mais determin√≠stico
      max_tokens: 512, // Respostas mais concisas
      top_p: 0.8,
      frequency_penalty: 0.1,
      presence_penalty: 0.1
    });

    const response = completion.choices[0].message.content;
    
    console.log(`‚úÖ OpenAI: Resposta gerada com sucesso (${response.length} caracteres)`);
    
    return response;
  }

  /**
   * Constr√≥i contexto estruturado com informa√ß√µes organizadas
   * @param {Object} searchResults - Resultados da busca h√≠brida
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Contexto estruturado
   */
  buildStructuredContext(searchResults, sessionHistory) {
    let context = `## CONTEXTO DA CONSULTA\n\n`;
    
    // Informa√ß√£o principal (Bot_perguntas)
    if (searchResults && searchResults.botPergunta) {
      context += `### INFORMA√á√ÉO PRINCIPAL
**Pergunta:** ${searchResults.botPergunta.pergunta}
**Resposta:** ${searchResults.botPergunta.resposta}
**Relev√¢ncia:** ${searchResults.botPergunta.relevanceScore || 'N/A'}/10
**Fonte:** Bot_perguntas (MongoDB)

`;
    }
    
    // Artigos relacionados
    if (searchResults && searchResults.articles && searchResults.articles.length > 0) {
      context += `### ARTIGOS RELACIONADOS\n`;
      searchResults.articles.forEach((article, index) => {
        context += `${index + 1}. **${article.title}**
   - Relev√¢ncia: ${article.relevanceScore || 'N/A'}/10
   - Conte√∫do: ${article.content.substring(0, 150)}...
   
`;
      });
    }
    
    // Hist√≥rico da sess√£o
    if (sessionHistory && sessionHistory.length > 0) {
      context += `### HIST√ìRICO DA CONVERSA\n`;
      sessionHistory.slice(-3).forEach(h => {
        context += `- ${h.role}: ${h.content}\n`;
      });
      context += `\n`;
    }
    
    return context;
  }

  /**
   * Constr√≥i o prompt otimizado com contexto e hist√≥rico
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento (legado)
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @param {Object} searchResults - Resultados da busca h√≠brida (novo)
   * @returns {string} Prompt formatado
   */
  buildOptimizedPrompt(question, context, sessionHistory, searchResults = null) {
    // Usar contexto estruturado se searchResults estiver dispon√≠vel
    const structuredContext = searchResults ? 
      this.buildStructuredContext(searchResults, sessionHistory) : 
      `### CONTEXTO DA BASE DE CONHECIMENTO
${context || "Nenhum contexto espec√≠fico encontrado."}

### HIST√ìRICO DE CONVERSA
${sessionHistory.length > 0 ? 
  sessionHistory.map(h => `${h.role}: ${h.content}`).join("\n") : 
  'Primeira pergunta da sess√£o.'}`;

    return `${structuredContext}
## PERGUNTA ATUAL
**Usu√°rio:** ${question}

## INSTRU√á√ïES
- Use APENAS as informa√ß√µes do contexto acima
- Se a pergunta for sobre cr√©dito, foco em prazos e processos
- Se for sobre documentos, liste exatamente o que √© necess√°rio
- Se for sobre prazos, seja espec√≠fico com datas e tempos
- Se n√£o houver informa√ß√£o suficiente, diga claramente

## RESPOSTA:`;
  }

  /**
   * Valida a qualidade da resposta gerada pela IA
   * @param {string} response - Resposta da IA
   * @param {string} question - Pergunta original
   * @returns {Object} Resultado da valida√ß√£o
   */
  validateResponse(response, question) {
    // Verificar se a resposta √© muito gen√©rica
    const genericResponses = [
      "n√£o encontrei essa informa√ß√£o",
      "n√£o tenho essa informa√ß√£o",
      "n√£o posso ajudar com isso",
      "n√£o sei",
      "n√£o consigo"
    ];
    
    const isGeneric = genericResponses.some(generic => 
      response.toLowerCase().includes(generic)
    );
    
    if (isGeneric && response.length < 50) {
      return {
        valid: false,
        reason: "Resposta muito gen√©rica",
        suggestion: "Buscar em outras fontes ou reformular pergunta"
      };
    }
    
    // Verificar se a resposta √© muito longa
    if (response.length > 500) {
      return {
        valid: false,
        reason: "Resposta muito longa",
        suggestion: "Resumir para m√°ximo 200 palavras"
      };
    }
    
    return { valid: true };
  }

  /**
   * Constr√≥i o prompt com contexto e hist√≥rico (m√©todo legado)
   * @param {string} question - Pergunta atual
   * @param {string} context - Contexto da base de conhecimento
   * @param {Array} sessionHistory - Hist√≥rico da sess√£o
   * @returns {string} Prompt formatado
   */
  buildPrompt(question, context, sessionHistory) {
    return this.buildOptimizedPrompt(question, context, sessionHistory);
  }

  /**
   * Verifica se a API OpenAI est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isOpenAIConfigured() {
    const configured = !!config.OPENAI_API_KEY && config.OPENAI_API_KEY !== 'your_openai_api_key_here';
    if (!configured) {
      console.warn('‚ö†Ô∏è AI Service: OpenAI n√£o configurado - OPENAI_API_KEY ausente ou inv√°lida');
    }
    return configured;
  }

  /**
   * Verifica se a API Gemini est√° configurada corretamente
   * @returns {boolean} Status da configura√ß√£o
   */
  isGeminiConfigured() {
    const configured = !!config.GEMINI_API_KEY && config.GEMINI_API_KEY !== 'your_gemini_api_key_here';
    if (!configured) {
      console.warn('‚ö†Ô∏è AI Service: Gemini n√£o configurado - GEMINI_API_KEY ausente ou inv√°lida');
    }
    return configured;
  }

  /**
   * Verifica se alguma API est√° configurada
   * @returns {boolean} Status da configura√ß√£o
   */
  isConfigured() {
    return this.isOpenAIConfigured() || this.isGeminiConfigured();
  }

  /**
   * Obt√©m status das configura√ß√µes de IA
   * @returns {Object} Status das configura√ß√µes
   */
  getConfigurationStatus() {
    return {
      gemini: {
        configured: this.isGeminiConfigured(),
        model: this.geminiModel,
        priority: 'primary'
      },
      openai: {
        configured: this.isOpenAIConfigured(),
        model: this.openaiModel,
        priority: 'fallback'
      },
      anyAvailable: this.isConfigured()
    };
  }

  /**
   * Verifica se o cache de status ainda √© v√°lido
   * @returns {boolean} Status do cache
   */
  _isCacheValid() {
    // Cache inv√°lido se n√£o h√° dados ou timestamp
    if (!this.statusCache.data || !this.statusCache.timestamp) {
      console.log('‚ö†Ô∏è AI Service: Cache inv√°lido - dados ou timestamp ausentes');
      return false;
    }
    
    // Verificar se cache expirou
    const now = Date.now();
    const cacheAge = now - this.statusCache.timestamp;
    const isValid = cacheAge < this.statusCache.ttl;
    
    if (!isValid) {
      console.log(`‚ö†Ô∏è AI Service: Cache expirado - idade: ${Math.round(cacheAge / 1000)}s, TTL: ${this.statusCache.ttl / 1000}s`);
    }
    
    return isValid;
  }

  /**
   * Ping HTTP para OpenAI (OTIMIZADO)
   * @returns {Promise<boolean>} Status da conex√£o
   */
  async _pingOpenAI() {
    if (!this.isOpenAIConfigured()) return false;
    
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 2000); // 2s timeout
      
      const response = await fetch('https://api.openai.com/v1/models', {
        method: 'HEAD',
        headers: { 
          'Authorization': `Bearer ${config.OPENAI_API_KEY}`,
          'User-Agent': 'VeloHub-Bot/1.0'
        },
        signal: controller.signal
      });
      
      clearTimeout(timeoutId);
      const isAvailable = response.ok;
      
      if (isAvailable) {
        console.log('‚úÖ OpenAI: Ping HTTP bem-sucedido');
      } else {
        console.warn(`‚ö†Ô∏è OpenAI: Ping HTTP falhou - Status: ${response.status}`);
      }
      
      return isAvailable;
    } catch (error) {
      if (error.name === 'AbortError') {
        console.warn('‚ö†Ô∏è OpenAI: Ping HTTP timeout (2s)');
      } else {
        console.warn('‚ö†Ô∏è OpenAI: Ping HTTP failed:', error.message);
      }
      return false;
    }
  }

  /**
   * Ping HTTP para Gemini (OTIMIZADO)
   * @returns {Promise<boolean>} Status da conex√£o
   */
  async _pingGemini() {
    if (!this.isGeminiConfigured()) return false;
    
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 2000); // 2s timeout
      
      const response = await fetch('https://generativelanguage.googleapis.com/v1/models', {
        method: 'HEAD',
        headers: { 
          'x-goog-api-key': config.GEMINI_API_KEY,
          'User-Agent': 'VeloHub-Bot/1.0'
        },
        signal: controller.signal
      });
      
      clearTimeout(timeoutId);
      const isAvailable = response.ok;
      
      if (isAvailable) {
        console.log('‚úÖ Gemini: Ping HTTP bem-sucedido');
      } else {
        console.warn(`‚ö†Ô∏è Gemini: Ping HTTP falhou - Status: ${response.status}`);
      }
      
      return isAvailable;
    } catch (error) {
      if (error.name === 'AbortError') {
        console.warn('‚ö†Ô∏è Gemini: Ping HTTP timeout (2s)');
      } else {
        console.warn('‚ö†Ô∏è Gemini: Ping HTTP failed:', error.message);
      }
      return false;
    }
  }

  /**
   * Teste inteligente de conex√£o com APIs de IA (OTIMIZADO)
   * @returns {Promise<Object>} Status das conex√µes
   */
  async testConnectionIntelligent() {
    // Verificar cache primeiro (TTL 3min)
    if (this._isCacheValid()) {
      console.log('‚úÖ AI Service: Usando cache de status das IAs (TTL 3min)');
      return this.statusCache.data;
    }
    
    console.log('üîç AI Service: Testando conex√µes das IAs (ping HTTP inteligente)');
    const startTime = Date.now();
    
    const results = {
      openai: { available: false, model: this.openaiModel, priority: 'primary' },
      gemini: { available: false, model: this.geminiModel, priority: 'fallback' },
      anyAvailable: false
    };

    // Teste PARALELO com ping HTTP (OTIMIZADO)
    try {
      const [openaiResult, geminiResult] = await Promise.allSettled([
        this._pingOpenAI(),
        this._pingGemini()
      ]);

      // Processar resultados
      results.openai.available = openaiResult.status === 'fulfilled' && openaiResult.value;
      results.gemini.available = geminiResult.status === 'fulfilled' && geminiResult.value;
      results.anyAvailable = results.openai.available || results.gemini.available;
      
      const endTime = Date.now();
      const duration = endTime - startTime;
      
      console.log(`‚ö° AI Service: Handshake inteligente conclu√≠do em ${duration}ms`);
      
    } catch (error) {
      console.error('‚ùå AI Service: Erro no handshake inteligente:', error.message);
    }
    
    // Atualizar cache (TTL 3min)
    this.statusCache.data = results;
    this.statusCache.timestamp = Date.now();
    
    // Logs assertivos sobre o resultado
    if (results.anyAvailable) {
      const primaryAI = results.openai.available ? 'OpenAI' : 'Gemini';
      const fallbackAI = results.openai.available && results.gemini.available ? 'Gemini' : 
                        results.gemini.available && results.openai.available ? 'OpenAI' : null;
      
      console.log(`‚úÖ AI Service: Cache atualizado (TTL 3min) - Prim√°ria: ${primaryAI}${fallbackAI ? `, Fallback: ${fallbackAI}` : ''}`);
    } else {
      console.error('‚ùå AI Service: NENHUMA API DE IA DISPON√çVEL - Verificar configura√ß√£o das chaves');
      console.error('‚ùå AI Service: OpenAI configurado:', this.isOpenAIConfigured());
      console.error('‚ùå AI Service: Gemini configurado:', this.isGeminiConfigured());
    }

    return results;
  }

  /**
   * Testa a conex√£o com as APIs de IA (M√âTODO LEGADO - MANTIDO PARA COMPATIBILIDADE)
   * @returns {Promise<Object>} Status das conex√µes
   */
  async testConnection() {
    // Verificar cache primeiro
    if (this._isCacheValid()) {
      console.log('‚úÖ AI Service: Usando cache de status das IAs');
      return this.statusCache.data;
    }
    
    console.log('üîç AI Service: Testando conex√µes das IAs (cache expirado ou inexistente)');
    const results = {
      openai: { available: false, model: this.openaiModel, priority: 'primary' },
      gemini: { available: false, model: this.geminiModel, priority: 'fallback' },
      anyAvailable: false
    };

    // Teste OpenAI (prim√°rio)
    if (this.isOpenAIConfigured()) {
      try {
        const openai = this._initializeOpenAI();
        if (openai) {
          const completion = await openai.chat.completions.create({
            model: this.openaiModel,
            messages: [{ role: "user", content: "Teste de conex√£o" }],
            max_tokens: 10,
          });
          results.openai.available = true;
          console.log('‚úÖ OpenAI: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå OpenAI: Erro no teste de conex√£o:', error.message);
      }
    }

    // Teste Gemini (fallback)
    if (this.isGeminiConfigured()) {
      try {
        const gemini = this._initializeGemini();
        if (gemini) {
          const model = gemini.getGenerativeModel({ model: this.geminiModel });
          const result = await model.generateContent("Teste de conex√£o");
          results.gemini.available = true;
          console.log('‚úÖ Gemini: Conex√£o testada com sucesso');
        }
      } catch (error) {
        console.error('‚ùå Gemini: Erro no teste de conex√£o:', error.message);
      }
    }

    results.anyAvailable = results.gemini.available || results.openai.available;
    
    // Atualizar cache
    this.statusCache.data = results;
    this.statusCache.timestamp = Date.now();
    
    // Logs assertivos sobre o resultado
    if (results.anyAvailable) {
      const primaryAI = results.openai.available ? 'OpenAI' : 'Gemini';
      const fallbackAI = results.openai.available && results.gemini.available ? 'Gemini' : 
                        results.gemini.available && results.openai.available ? 'OpenAI' : null;
      
      console.log(`‚úÖ AI Service: Cache atualizado - Prim√°ria: ${primaryAI}${fallbackAI ? `, Fallback: ${fallbackAI}` : ''}`);
    } else {
      console.error('‚ùå AI Service: NENHUMA API DE IA DISPON√çVEL - Verificar configura√ß√£o das chaves');
      console.error('‚ùå AI Service: OpenAI configurado:', this.isOpenAIConfigured());
      console.error('‚ùå AI Service: Gemini configurado:', this.isGeminiConfigured());
    }

    return results;
  }
}

module.exports = new AIService();
